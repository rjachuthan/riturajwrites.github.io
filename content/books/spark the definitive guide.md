---
title: "Spark: The Definitive Guide"
description: ""
aliases: 
tags:
  - books
  - dataengineering
  - literature-note
draft: false
date: 2024-04-28
status: in-progress
---

<center>
	<img src="https://images-na.ssl-images-amazon.com/images/S/compressed.photo.goodreads.com/books/1518177736i/38467996.jpg" />
</center>

> [!INFO] Metadata
>  - **Full Title**: "Spark: The Definitive Guide: Big Data Processing Made Simple"
>  - **Author**: [[Bill Chambers]], [[Matei Zaharia]]
>  - **Goodreads URL**: [Spark: The Definitive Guide: Big Data Processing made Simple](https://www.goodreads.com/book/show/38467996-spark?ac=1&from_search=true&qid=YxSBVyJBud&rank=1)

## Highlights

### What is Apache Spark?

[[apache spark|Apache Spark]] stands as a unified computing engine alongside a suite of libraries designed for parallel data processing across computer clusters. It was made to deal with changes in how computers and data storage work. Back in 2005, computer hardware stopped getting much faster, so people needed new ways to handle lots of tasks at once. At the same time, storing data became cheaper, which meant there was more data to deal with than ever before.

### Apache Spark's Philosophy

- **Unified**: Spark offers a single platform for various big data tasks like loading data, querying with [[SQL]], [[Machine Learning]], and handling real-time data, making development easier and more efficient.
- **Computing Engine**: Spark focuses on computing tasks, working with different storage systems but not storing data permanently itself. It aims to minimise data movement and costs by computing where data resides.
- **Libraries**: Spark provides additional libraries for various tasks, expanding its capabilities and allowing users to work with different types of data and perform specific tasks more easily. You can find Spark's libraries listed [here](https://spark-packages.org/).
