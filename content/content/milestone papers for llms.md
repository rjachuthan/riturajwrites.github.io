---
title: Milestone Papers For LLMs
description: ""
aliases:
  - Milestone Papers For LLMs
tags:
  - ðŸŒ±seedling
draft: true
date: 2024-06-26
backlink:
  - "[[llm|Large Language Model]]"
---

> [!timeline|red] June 2017
>
> ### [[transformer|Transformer]] by Google
>
> Research Paper Link: [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

> [!timeline|red] June 2018
>
> ### GPT by OpenAI
>
> Research Paper Link: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

> [!timeline|red] October 2018
>
> ### BERT by Google
>
> Research Paper Link: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

> [!timeline|red] August 2019
>
> ### XLNet by Google/CMU
>
> Research Paper Link: [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)

> [!timeline|red] September 2019
>
> ### RoBERTa by Facebook
>
> Research Paper Link: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)

> [!timeline|red] October 2019
>
> ### T5 by Google
>
> Research Paper Link: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)

> [!timeline|red] May 2020
>
> ### GPT-3 by OpenAI
>
> Research Paper Link: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)

> [!timeline|red] September 2020
>
> ### BigGAN by Google/DeepMind
>
> Research Paper Link: [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/pdf/1809.11096.pdf)

> [!timeline|red] December 2020
>
> ### Dall-E by OpenAI
>
> Research Paper Link: [Zero-Shot Text-to-Image Generation](https://cdn.openai.com/dall-e/paper.pdf)

> [!timeline|red] December 2020
>
> ### CLIP by OpenAI
>
> Research Paper Link: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)

> [!timeline|red] January 2021
>
> ### Codex by OpenAI
>
> Research Paper Link: [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)

> [!timeline|red] February 2021
>
> ### EleutherAI GPT-Neo by EleutherAI
>
> Research Paper Link: [GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow](https://arxiv.org/pdf/2104.01655.pdf)

> [!timeline|red] May 2021
>
> ### Switch Transformer by Google
>
> Research Paper Link: [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf)

> [!timeline|red] July 2021
>
> ### LaMDA by Google
>
> Research Paper Link: [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf)

> [!timeline|red] October 2021
>
> ### Gopher by DeepMind
>
> Research Paper Link: [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/pdf/2112.11446.pdf)

> [!timeline|red] April 2022
>
> ### Chinchilla by DeepMind
>
> Research Paper Link: [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)

> [!timeline|red] August 2022
>
> ### OPT by Meta
>
> Research Paper Link: [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)

> [!timeline|red] December 2022
>
> ### Galactica by Meta
>
> Research Paper Link: [Galactica: A Large Language Model for Science](https://arxiv.org/pdf/2211.09085.pdf)

> [!timeline|red] February 2023
>
> ### Claude by Anthropic
>
> Research Paper Link: [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073.pdf)

> [!timeline|blue] March 2023
>
> ### GPT-4 by OpenAI
>
> Research Paper Link: [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf)
